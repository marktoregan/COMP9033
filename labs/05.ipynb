{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9033 - Data Analytics Lab 05: Data modelling and model selection\n",
    "## Introduction\n",
    "\n",
    "This week's lab is focused on data modelling and model selection. At the end of the lab, you should be able to use `scikit-learn` to:\n",
    "\n",
    "- Create a rule-based spam classification model for SMS messages.\n",
    "- Predict whether a given SMS message is spam or not.\n",
    "- Measure the accuracy of the model.\n",
    "- Select the best model from a set of given candidates.\n",
    "\n",
    "### Getting started\n",
    "\n",
    "Let's start by importing the packages we'll need. As usual, we'll import `pandas` for exploratory analysis, but this week we're also going to use `scikit-learn` (`sklearn`), a modelling and machine learning library for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the data. Write the path to your sms.csv file in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = 'data/sms.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to load the CSV data into a pandas data frame with the columns `label` and `message`.\n",
    "\n",
    "> **Note:** This week, the CSV file is not comma separated, but instead tab separated. We can tell `pandas` about the different format using the `sep` argument, as shown in the cell below. For more information, see the `read_csv` [documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sms = pd.read_csv(data_file, sep='\\t', header=None, names=['label', 'message'])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a spam classifier\n",
    "\n",
    "Let's build a rule-based model that classifies the SMS messages as either *spam* or *ham* by matching the content of the message against a known list of spam phrases. The cell below defines a custom `scitkit-learn` classifier that accepts an optional list of spam phrases to match against (the `spam` argument) and a optional boolean (the `lowercase` argument) that defines whether the messages should be converted to lowercase before comparison with the phrases in the list.\n",
    "\n",
    "> **Note:** `scikit-learn` doesn't support generic rule-based models out of the box, so we have to code our own. However, it does support many different kinds of machine learning models, so we won't usually have to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PhraseClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, spam=[], lowercase=False):\n",
    "        self.spam = spam\n",
    "        self.lowercase = lowercase\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        results = []\n",
    "        for message in X:\n",
    "            message = message.lower() if self.lowercase else message\n",
    "            results.append(self.spam_or_ham(message))\n",
    "        return results\n",
    "\n",
    "    def spam_or_ham(self, message):\n",
    "        # Start out assuming we have ham\n",
    "        result = 'ham'\n",
    "        \n",
    "        # If any of the phrases in self.spam match, then mark the message as spam\n",
    "        for phrase in self.spam:\n",
    "            if phrase in message:\n",
    "                result = 'spam'\n",
    "                break\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        y_pred = self.predict(X, y)\n",
    "        return sum([1 if y1 == y1 else 0 for y1, y2 in zip(y, y_pred)]) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention, when we build a predictive model using `scikit-learn`, we separate our data into two variables, `X` and `y`:\n",
    "\n",
    "- `X`: This represents the data to train on or evaluate, i.e. it is a matrix consisting of explanatory variables / features.\n",
    "- `y`: This represents the true values of the quantity we are trying to predict, i.e. it is the target.\n",
    "\n",
    "For instance, if we wanted to predict ice cream sales based on temperature, then we would assign the temperature data (an explanatory variable) to the `X` variable and the ice cream sales data (the target variable) to the `y` variable.\n",
    "\n",
    "As we are trying to predict the label (spam or ham) of SMS messages based on their content, we'll assign the messages (the explanatory variables) to the `X` variable and the labels (the target variables) to the `y` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = sms['message']\n",
    "y = sms['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build our first classifier. Let's create one that marks every message as ham to start with (i.e. one with no spam phrases). It won't be very useful, but it will give us a baseline accuracy from which to improve on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = PhraseClassifier() # clf is short for classifier\n",
    "y_pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure how well our classifier works, let's create a classification report using the true labels for the SMS messages (`y`) and the predicted labels we've just created (`y_pred`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we haven't covered some of the terms here in class yet, they're not difficult to understand:\n",
    "\n",
    "- **Precision:** The proportion of the classifications that were correct (e.g. correctly predicted hams / total predicted hams). Ideally, precision is 100%.\n",
    "- **Recall:** The proportion of a class that was correctly predicted (e.g. correctly predicted hams / total actual hams). Ideally, recall is 100%.\n",
    "- **F1 score:** The harmonic mean of precision and recall. This acts as a \"unified\" measure of accuracy. Ideally, the F1 score is 100%.\n",
    "- **Support:** The number of samples of the given class (e.g. the numbers of hams and spams).\n",
    "\n",
    "We can interpret the results above like this:\n",
    "\n",
    "- 87% of the messages we labelled as ham were actually ham (precision for ham = 0.87).\n",
    "- None of the messages we labelled as spam were actually spam (precision for spam = 0.00).\n",
    "- We labelled every actual ham as ham (recall for ham = 1.00).\n",
    "- We labelled no actual spam as spam (recall for spam = 0.00).\n",
    "- We made predictions for 5572 messages, 4825 of which were ham and 747 of which were spam.\n",
    "\n",
    "This isn't surprising, given that our classifier labels every message as ham and ~87% of our dataset (4825/5572) is ham.\n",
    "\n",
    "Let's make an improvement by adding the spam phrases \"urgent\" and \"win\". This is easy to do: we just have to create a new classifier and use it to make some new predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = PhraseClassifier(spam=['urgent', 'win'])\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "print classification_report(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the results have improved:\n",
    "\n",
    "- 88% of the messages we labelled as ham were actually ham (precision for ham = 0.88).\n",
    "- 51% of the messages we labelled as spam were actually spam (precision for spam = 0.51).\n",
    "- We labelled 99% of actual ham as ham (recall for ham = 0.99).\n",
    "- We labelled 9% of actual spam as spam (recall for spam = 0.09).\n",
    "\n",
    "Our overall F1 score has also increased (from 0.80 to 0.82), which indicates that our model is performing better in general.\n",
    "\n",
    "The `PhraseClassifier` algorithm also accepts a boolean `lowercase` keyword argument. When it's set to true, messages are converted to lowercase before being compared with our list of spam phrases. This way, we can catch messages like \"WINNER!! ...\" as well as \"SIX chances to win...\". Let's see what effect this has on our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = PhraseClassifier(spam=['urgent', 'win'], lowercase=True)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "print classification_report(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this has improved our spam detector!\n",
    "\n",
    "## Model selection\n",
    "\n",
    "So far, we've chosen phrases that are good indicators of spam. But if we include a word that isn't such a good indicator, our model becomes worse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = PhraseClassifier(spam=['urgent', 'win', 'hi'], lowercase=True)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "print classification_report(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice if we choose a set of spam keywords that *maximized* the performance of our model. We can do this using model selection, i.e. building a set of different candidate models and choosing the best one. This is easy to do with `scikit-learn`!\n",
    "\n",
    "We can start by splitting the data into training and testing sets using the [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method from `scikit-learn`. Let's use 60% of the data for training and 40% of the data for testing (i.e. `test_size=0.4`) and *stratify* our split so that there are roughly the same proportions of spam in the training and testing sets.\n",
    "\n",
    "> **Note:** In the cell below, we also set `random_state=0` so that the split occurs the same way each time. This is just so this notebook runs the same way on different computers and everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sms['message'], sms['label'], test_size=0.5, stratify=sms['label'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_test_split` method *randomly* selects data from its inputs to create four different variables:\n",
    "\n",
    "- `X_train`: The training data set.\n",
    "- `X_test`: The testing data set.\n",
    "- `y_train`: The true class labels of the records in the training set (i.e. spam or ham).\n",
    "- `y_test`: The true class labels of the records in the testing set.\n",
    "\n",
    "Now that we have our training and testing data, let's define a set of parameters from which to build different models. Let's start by creating a sorted list of the most popular words in the spam messages in our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list of the words in the training set messages that are labelled as spam\n",
    "spam_words = [word.lower() for msg in X_train[y_train == 'spam'] for word in msg.split()]\n",
    "\n",
    "# Order the spam words by popularity\n",
    "top_spam_words = pd.Series(spam_words).value_counts().index\n",
    "\n",
    "# Print the top ten\n",
    "print top_spam_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a lot of the most popular words are ones we commonly use. These aren't good indicators of spam in general, so let's remove them. `scikit-learn` defines a set of *stop words*, i.e. words that are so commonly used that they don't indicate anything in particular. Let's remove these from our set of most popular words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_spam_words = [word for word in top_spam_words if word not in stop_words.ENGLISH_STOP_WORDS]\n",
    "\n",
    "print top_spam_words[:10] # Print the top ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a better set of words to use. Let's make a list of all the possible different combinations of the most popular of these words (a brute force approach) using Python's [`itertools.combinations`](https://docs.python.org/2/library/itertools.html#itertools.combinations) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidates = top_spam_words[:10]\n",
    "combinations = [combination for n in range(1, len(candidates)) for combination in itertools.combinations(candidates, n)]\n",
    "\n",
    "print \"Total combinations: %d\" % len(combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a set of parameters to build models for. The set is just a Python dictionary, where the keys match the arguments of the classifier we're using and the values represent different choices that can be made for a particular key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'spam': combinations, # Try every combination\n",
    "    'lowercase': [True, False] # Try setting lowercase True and False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) class from `scikit-learn` to build every possible model defined by the parameters and choose the best one (i.e. to do model selection). Generally, when we use `GridSearchCV`, we will specify three parameters:\n",
    "\n",
    "1. A model-building algorithm.\n",
    "2. The set of parameters to use to build models.\n",
    "3. An internal cross validation technique to use to measure the accuracy of the models built using the parameters.\n",
    "\n",
    "In this case, we will use `PhraseClassifier` to build the model and the set of parameters above to generate the different configurations. We can set the cross validation technique to use using the `cv` keyword argument (see the [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) documentation for more information on how this works). Let's set `cv=5` to use $K$-fold cross validation with $K=5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(PhraseClassifier(), parameters, cv=5)\n",
    "clf.fit(X_train, y_train) # Fit using the training set\n",
    "\n",
    "y_pred = clf.predict(X_test) # Predict using the test set\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Best parameters: %s' % clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the *best* spam classifier that can be generated from the top ten spam words is the one that simply checks whether a messages contains the word \"free\" or not. If we had enough resources (time, compute power), we could determine the best spam classifier based on *all* of the words in the spam messages, not just the top ten."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
