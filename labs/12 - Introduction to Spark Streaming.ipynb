{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# COMP9033 - Data Analytics Lab 12: Introduction to Spark Streaming\n",
    "## Introduction\n",
    "\n",
    "In this lab, we're going to look at data streaming with Apache Spark. At the end of the lab, you should be able to:\n",
    "\n",
    "- Create a local `StreamingContext` object.\n",
    "- Use Spark to analyse the recent Wikipedia edits stream.\n",
    "\n",
    "### Getting started\n",
    "\n",
    "Let's start by importing the packages we'll need. This week, we'll need to install the `sseclient` package so we can connect to the Wikipedia stream. This package is *not* installed on student vDesktop environments, but you can install it if you're running at home or using [Docker](https://github.com/jupyter/docker-stacks/tree/master/pyspark-notebook) by executing the code in the box below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install sseclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like last week, we're going to use `pyspark`, a Python package that wraps Apache Spark and makes its functionality available in Python. We'll also use a few of the standard Python libraries - `json`, `socket`, `threading` and `time` - as well as the `sseclient` package you just installed to connect to the event stream.\n",
    "\n",
    "> **Note:** You don't need to understand how these packages are used to connect to the event stream, but the code is below if you're curious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pyspark\n",
    "import socket\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "from sseclient import SSEClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Wikipedia events\n",
    "\n",
    "Currently, Spark supports three kinds of streaming connection out of the box:\n",
    "\n",
    "1. Apache Kafka\n",
    "2. Amazon Kinesis\n",
    "3. Apache Flume\n",
    "\n",
    "While it's possible to connect to other kinds of streams too, we must write our own code to do it and, at present, this is unsupported in Python (although it is possible in Java and Scala). However, Spark also supports streaming data from arbitrary TCP socket endpoints and so we can instead relay the remote data stream to a local socket port to enable Spark to consume it.\n",
    "\n",
    "The code in the box below connects to the Wikipedia event stream and publishes its content to a local port. While you don't need to understand it to complete the lab, the basic logic is as follows:\n",
    "\n",
    "1. Connect to the Wikipedia RecentChanges stream using SSEClient.\n",
    "2. Create a local socket connection on port 50000.\n",
    "3. When a client (e.g. Spark) connects to the local socket, relay the next available event to it from the event stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def relay():\n",
    "    events = SSEClient('https://stream.wikimedia.org/v2/stream/recentchange')\n",
    "    \n",
    "    s = socket.socket()\n",
    "    s.bind(('localhost', 50000))\n",
    "    s.listen(1)\n",
    "    while True:\n",
    "        try:\n",
    "            client, address = s.accept()\n",
    "            for event in events:\n",
    "                if event.event == 'message':\n",
    "                    client.sendall(event.data)\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "        finally:\n",
    "            client.close()\n",
    "    \n",
    "\n",
    "threading.Thread(target=relay).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Streaming analysis\n",
    "\n",
    "Now that we have our stream relay set up, we can start to analyse its contents. First, let's initialise a [`SparkContext`](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.SparkContext) object, which will represent our connection to the Spark cluster. To do this, we must first specify the URL of the master node to connect to. As we're only running this notebook for demonstration purposes, we can just run the cluster locally, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(master='local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a [`StreamingContext`](https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext) object, which represents the streaming functionality of our Spark cluster. When we create the context, we must specify a batch duration time (in seconds), to tell Spark how often it should process data from the stream. Let's process the Wikipedia data in batches of one second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our `StreamingContext` object, we can create a data stream from our local TCP relay socket with the [`socketTextStream`](https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.socketTextStream) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stream = ssc.socketTextStream('localhost', 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we've created a data stream, nothing happens! Before Spark starts to consume the stream, we must first define one or more operations to perform on it. Let's count the number of edits made by different users in the last minute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users = (\n",
    "    stream.map(json.loads)                   # Parse the stream data as JSON\n",
    "          .map(lambda obj: obj['user'])      # Extract the values corresponding to the 'user' key\n",
    "          .map(lambda user: (user, 1))       # Give each user a count of one\n",
    "          .window(60)                        # Create a sliding window, sixty seconds in length\n",
    "          .reduceByKey(lambda a, b: a + b)   # Reduce all key-value pairs in the window by adding values\n",
    "          .transform(                        # Sort by the largest count\n",
    "              lambda rdd: rdd.sortBy(lambda kv: kv[1], ascending=False))\n",
    "          .pprint()                          # Print the results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, nothing happens! This is because the `StreamingContext` must be started before the stream is processed by Spark. We can start data streaming using the [`start`](https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.start) method of the `StreamingContext` and stop it using the [`stop`](https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.stop) method. Let's run the stream for two minutes (120 seconds) and then stop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "\n",
    "time.sleep(120)\n",
    "\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, Spark counts the number of edits made by each user in the past sixty seconds and emits updates once per second (the original batch duration of the `StreamingContext`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
