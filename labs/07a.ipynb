{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# COMP9033 - Data Analytics Lab 07a: $k$ nearest neighbours classification\n",
    "## Introduction\n",
    "\n",
    "This lab focuses on SMS message spam detection using $k$ nearest neighbours classification. It's a direct counterpart to the rule-based spam detection from Lab 05. At the end of the lab, you should be able to use `scikit-learn` to:\n",
    "\n",
    "- Create a $k$ nearest neighbours classification model.\n",
    "- Use the model to predict new values.\n",
    "- Measure the accuracy of the model.\n",
    "\n",
    "### Getting started\n",
    "\n",
    "Let's start by importing the packages we'll need. This week, we're going to use the `neighbors` subpackage from `scikit-learn` to build k nearest neighbours models. We'll also use the `dummy` package to build a baseline model from we which can gauge how good our final model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, let's load the data. Write the path to your `sms.csv` file in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_file = 'data/sms.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Execute the cell below to load the CSV data into a pandas data frame with the columns `label` and `message`.\n",
    "\n",
    "> **Note:** This week, the CSV file is not comma separated, but instead tab separated. We can tell `pandas` about the different format using the `sep` argument, as shown in the cell below. For more information, see the `read_csv` [documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms = pd.read_csv(data_file, sep='\\t', header=None, names=['label', 'message'])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data modelling\n",
    "\n",
    "Before we start modelling, let's split our data into training and test so we can measure the accuracy of our final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = sms['message']\n",
    "y = sms['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data transformation\n",
    "\n",
    "Our data is in the form of raw text. This was fine when we were using a rule-based model, but it won't work with $k$ nearest neighbours. Instead, we'll need to transform the data into a numerical representation. One popular way to do this with text data is to compute *term frequency* (TF) and *inverse document frequency* (IDF) measures:\n",
    "\n",
    "- Term frequency is a measure of how often a given term appears in a given document, e.g. how often the word \"free\" appears in a given SMS message. The more often a word appears in a document, the higher its term frequency.\n",
    "- Inverse document frequency is a measure of how rare a word is in a set of documents, e.g. the word \"the\" appears commonly in many SMS messages and so its presence (or absence) provides little information when it comes to distinguishing spam from ham. The higher the inverse document frequency of a word, the rarer it is (and, therefore, the more distinguishing power it has).\n",
    "\n",
    "Typically, term frequency and inverse document frequency are combined as a single metric, [*term frequency-inverse document frequency*](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (TF-IDF), which is simply the multiple of the individual values. Consequently, if a term has a high TF-IDF score, its presence across a set of documents (e.g. SMS messages) is low, while its number of occurrences in a given document (e.g. a candidate SMS message under evaluation) is high. If a term has a low TF-IDF score, this is an indicator that it doesn't appear very frequently in a given document, occurs very frequently across the set of documents, or both. We can exploit this information to find terms that can distinguish a certain set of documents (e.g. spam) from a larger set of documents.\n",
    "\n",
    "\n",
    "### Dummy modelling\n",
    "\n",
    "Before we build the $k$ nearest neighbours model, let's build a dummy model, i.e. a naive model that predicts new values using a simple strategy. Dummy models are usually not good predictors (we usually won't use them to solve real problems), but are useful because they provide a baseline accuracy measurement for the data set, from which we can gauge the accuracy of any further models we build. In Lab 05, we built a rule-based model for SMS message spam detection for this purpose.\n",
    "\n",
    "`scikit-learn` provides dummy model functionality via the [`dummy`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.dummy) subpackage. This subpackage provides both dummy regression and classification algorithms, which can be customised with different strategies. We can use the [`DummyClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier) class to build a dummy classification model. `DummyClassifier` supports five different strategies for predicting values:\n",
    "\n",
    "1. `strategy='stratified'`: Predict new values randomly, but in proportion to their frequency in the training set.\n",
    "2. `strategy='most_frequent'`: Predict new values as the most frequently occurring target variable in the training set.\n",
    "3. `strategy='prior'`: Predict new values as the most frequently occurring target variable in the training set. This is essentially the same as `strategy='most_frequent'`, but returns different values when the `predict_proba` method is called.\n",
    "4. `strategy='uniform'`: Predict new values randomly, with equal probability.\n",
    "5. `strategy='constant'`: Predict new values as some constant value (the constant value must also be specified using the `constant` keyword argument).\n",
    "\n",
    "Let's build a model that predicts new values in proportion to their occurrence in the training set. As usual, we create an instance of the model building class and then use the `fit` method to fit it to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.86      0.86      0.86      2413\n",
      "       spam       0.12      0.12      0.12       373\n",
      "\n",
      "avg / total       0.76      0.76      0.76      2786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(stop_words=stop_words.ENGLISH_STOP_WORDS), # Remove stop words before computing TF-IDF\n",
    "    DummyClassifier(strategy='stratified')\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As can be seen, the dummy model is a poor fit for the data and performs worse than our simple rule-based model from Lab 05. But it does give us a baseline error level from which we can improve. Let's build a $k$ nearest neighbours model and see what the difference is.\n",
    "\n",
    "### $k$ nearest neighbours modelling\n",
    "\n",
    "Let's use model selection via cross validation to select the optimal $k$ nearest neighbours model from a set of candidates:\n",
    "\n",
    "> **Note:** The grid search could take a few minutes to complete, depending on the amount of processing power  you have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smoo...owski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'kneighborsclassifier__n_neighbors': [1, 2, 3, 4, 5, 10, 15, 20], 'kneighborsclassifier__weights': ['uniform', 'distance'], 'kneighborsclassifier__metric': ['manhattan', 'euclidean']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(stop_words=stop_words.ENGLISH_STOP_WORDS),\n",
    "    KNeighborsClassifier()\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'kneighborsclassifier__n_neighbors': [1, 2, 3, 4, 5, 10, 15, 20],\n",
    "    'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
    "    'kneighborsclassifier__metric': ['manhattan', 'euclidean']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1) # n_jobs=-1 uses all available CPUs for calculation\n",
    "gs.fit(X_train, y_train) # Fit using the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can check the parameters of the selected model using the `best_params_` attribute of the fitted grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kneighborsclassifier__metric': 'euclidean',\n",
       " 'kneighborsclassifier__n_neighbors': 1,\n",
       " 'kneighborsclassifier__weights': 'uniform'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, we can use our model to predict the classes of our test set data and print a classification report to compare the results to the dummy model above and the rule-based model from Lab 05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.92      1.00      0.96      2413\n",
      "       spam       1.00      0.47      0.64       373\n",
      "\n",
      "avg / total       0.93      0.93      0.92      2786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model is much more accurate than both the dummy model above and the rule-based model from Lab 05. Specifically, we can say that:\n",
    "\n",
    "- 92% of the messages we labelled as ham were actually ham (precision for ham = 0.92).\n",
    "- 100% of the messages we labelled as spam were actually spam (precision for spam = 1.00).\n",
    "- We labelled every actual ham as ham (recall for ham = 1.00).\n",
    "- We labelled 47% of spam as spam (recall for spam = 0.47).\n",
    "\n",
    "While no ham was misclassified as spam, we only managed to filter 47% of spam emails (approximately one in every two). Next week, we'll use decision tree classifiers to improve on this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
