{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# COMP9033 - Data Analytics Lab 10: Association rule mining\n",
    "## Introduction\n",
    "\n",
    "In this lab, we're going to look at association rule mining for grocery data. At the end of the lab, you should be able to:\n",
    "\n",
    "- Find frequently occurring itemsets using the FPGrowth algorithm.\n",
    "- Compute the support of an association rule.\n",
    "- Compute the confidence of an association rule.\n",
    "- Sort association rules by support and confidence.\n",
    "\n",
    "### Getting started\n",
    "\n",
    "Let's start by importing the packages we'll need. As usual, we'll import `pandas` for exploratory analysis, but this week we're also going to use `pyspark`, a Python package that wraps Apache Spark and makes its functionality available in Python. Spark also supports frequent itemset generation using the FPGrowth algorithm, so we'll import this functionality too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "from pyspark.mllib.fpm import FPGrowth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, let's initialise a [`SparkContext`](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.SparkContext) object, which will represent our connection to the Spark cluster. To do this, we must first specify the URL of the master node to connect to. As we're only running this notebook for demonstration purposes, we can just run the cluster locally, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(master='local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> **Note:** By specifying `master='local[*]'`, we are instructing Spark to run with as many worker threads as there are logical cores available on the host machine. Alternatively, we could directly specify the number of threads, e.g. `master='local[4]'` to run four threads. However, we need to make sure to specify at least *two* threads, so that there is one available for resource management and at least one available for data processing.\n",
    "\n",
    "Next, let's load the data. Write the path to your `groceries.csv` file in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = 'data/groceries.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can load the data using the [`textFile`](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.SparkContext.textFile) method of the `SparkContext` we have created, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "transactions = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "However, `textFile` only has the effect of loading the raw data into a [Spark RDD](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD). Unlike `pandas`, it doesn't parse the CSV structure of the file and extract the individual fields. We can see this directly by examining the first few entries of the RDD using its [`take`](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.take) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "transactions.take(5) # Take the first five entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As can be seen, the data consists of a collection of transactions from a supermarket, where each row corresponds to a transaction and the items in a row correspond to the items that were purchased in that transaction.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "The rows in the `transactions` RDD consist of the raw string data from the `groceries.csv` file, i.e. each row is an unparsed CSV string. Before we can mine frequent itemsets, we'll need to parse these strings into lists of items.\n",
    "\n",
    "Parsing CSV strings is a relatively trivial operation in standard Python - we just need to call the [`split`](https://docs.python.org/2/library/stdtypes.html#str.split) method. For instance, we can split the string `'hello,world'` into the list of strings `['hello', 'world']` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "my_csv = 'hello,world'\n",
    "my_csv.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To apply the same logic to the data in the `transactions` RDD, all we need to do is apply the `split` function to each line. We can do this using the [`map`](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.map) method of the RDD, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "transactions = transactions.map(lambda line: line.split(','))\n",
    "transactions.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Association rule mining\n",
    "\n",
    "Next, let's mine our transaction data to find interesting dependencies between itemsets. While we could do this in a brute force manner, as the number of transactions and items grows larger, the approach becomes significantly more computationally demanding. Instead, let's use frequent itemset generation to first generate a set of frequently occurring sets of items and then mine these for association rules. This approach is much more computationally efficient than a brute force strategy.\n",
    "\n",
    "### Frequent itemset generation\n",
    "\n",
    "While there are a number of approaches available for mining frequently occuring itemsets (e.g. Apriori, Eclat), Spark supports the [`FPGrowth`](https://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.fpm.FPGrowth) algorithm directly. To run the algorithm on our set of transactions, we need to specify two parameters:\n",
    "\n",
    "1. `minSupport`: A minimum support threshold, used to filter out itemsets that don't occur frequently enough.\n",
    "2. `numPartitions`: The number of partitions used by parallel FPGrowth (affects performance only, not accuracy).\n",
    "\n",
    "Let's set the minimum support level at 1% and use ten partitions. We can then train a frequent itemset model using the [`train`](https://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.fpm.FPGrowth.train) method of the `FPGrowth` class, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = FPGrowth.train(transactions, minSupport=0.01, numPartitions=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can extract the most frequent itemsets from the model using its [`freqItemsets`](https://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.fpm.FPGrowthModel.freqItemsets) method, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "itemsets = model.freqItemsets().collect() # Call collect to reduce the RDD to a Python list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's examine the results of the modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Found %d frequent itemsets' % len(itemsets))\n",
    "itemsets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As can be seen, the FPGrowth algorithm has identified 332 frequent itemsets in the transaction history. Each itemset is represented by a [`FreqItemset`](https://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html#pyspark.mllib.fpm.FPGrowth.FreqItemset) object, which has two properties:\n",
    "\n",
    "1. `items`: The set of items in the frequently occurring itemset.\n",
    "2. `freq`: The number of times the set of items has occurred in the transaction history.\n",
    "\n",
    "### Association rule mining\n",
    "\n",
    "Association rule mining is supported by Spark in both its Java and Scala APIs. However, Python support is not currently available (although is due in the next release of Spark).\n",
    "\n",
    "Instead, let's compute the association rules directly using Python. This involves a little more coding than in previous labs, but the general idea is the same as covered in class. To start, let's create a map of each frequent itemset to its count in the transaction history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "counts_x = {tuple(sorted(itemset.items)): itemset.freq for itemset in itemsets} # items -> frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This dictionary simply maps the items in each frequent itemset to the number of times those items have appeared in the transaction history, i.e. it measures $\\text{count}(X)$ for each frequent itemset $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "counts_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, let's generate every possible antecedent-consequent pair that can be made from the frequent itemsets and map them to their frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "counts_xy = {}\n",
    "for itemset in itemsets:\n",
    "    items = sorted(itemset.items)\n",
    "    for i in range(len(items)):\n",
    "        for x in itertools.combinations(items, i+1):\n",
    "            y = tuple(sorted([i for i in items if i not in x]))\n",
    "            if y:\n",
    "                counts_xy.setdefault(x, {})\n",
    "                counts_xy[x][y] = itemset.freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This dictionary simply maps all possible subsets of the frequent itemsets to their frequencies in the transaction history, i.e. it measures $\\text{count}(X \\cup Y)$ for each possible antecedent $X$ and consequent $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "counts_xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, we can generate a set of association rules by compute the support and confidence for each antecedent-consequent pair, i.e.\n",
    "\n",
    "\\begin{align}\n",
    "\\def \\tcount{\\text{count}}\n",
    "\\def \\support{\\text{support}}\n",
    "\\def \\confidence{\\text{confidence}}\n",
    "\\support(X \\Rightarrow Y) = \\frac{\\tcount(X \\cup Y)}{n},\\\\\n",
    "\\\\\n",
    "\\confidence(X \\Rightarrow Y) = \\frac{\\tcount(X \\cup Y)}{\\tcount(X)}.\n",
    "\\end{align}\n",
    "\n",
    "Using the `counts_x` and `counts_xy` dictionaries above, this is quite simple to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n = transactions.count() # Compute the total number of transations\n",
    "\n",
    "rules = []\n",
    "for x in counts_xy:\n",
    "    count_x = counts_x[x]\n",
    "\n",
    "    for y in counts_xy[x]:\n",
    "        count_xy = counts_xy[x][y]\n",
    "\n",
    "        support = 1.0 * count_xy / n\n",
    "        confidence = 1.0 * count_xy / count_x\n",
    "\n",
    "        rules.append({'antecedent': x,\n",
    "                      'consequent': y,\n",
    "                      'support': support,\n",
    "                      'confidence': confidence})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can use `pandas` to more easily explore the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rules, columns=['antecedent', 'consequent', 'support', 'confidence'])\n",
    "df.sort_values(['support', 'confidence'], ascending=False, inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As can be seen, many of rules with the highest level of support and confidence relate to whole milk. This is partly because of the distribution of different kinds of items in the transaction history, i.e. relatively few occur very commonly while most occur very irregularly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "item_counts = transactions.flatMap(lambda item: item). \\\n",
    "                           map(lambda item: (item, 1)). \\\n",
    "                           reduceByKey(lambda x, y: x + y). \\\n",
    "                           collectAsMap()\n",
    "item_counts = pd.Series(item_counts)\n",
    "\n",
    "item_counts.hist(bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We could attempt to merge some of the more rarely occuring items (e.g. `'baby food'`) with other rarely occuring but related items (e.g. `'baby cosmetics'`) to force the algorithm to generate more meaningful rules (i.e. use feature transformation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "item_counts.sort_values(ascending=True).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
